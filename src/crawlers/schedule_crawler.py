"""
KBO Schedule Crawler POC
Collects game IDs from the KBO schedule page
"""
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page

from src.utils.team_codes import team_code_from_game_id_segment


class ScheduleCrawler:
    """KBO ê³µì‹ ì‚¬ì´íŠ¸ì˜ ì›”ë³„ ê²½ê¸° ì¼ì • í˜ì´ì§€ì—ì„œ ê²½ê¸° ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - íŠ¹ì • ì—°ë„ì™€ ì›”ì— í•´ë‹¹í•˜ëŠ” ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
    - í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ê²½ê¸° ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ìœ  ID(gameId)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - gameIdë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ê¸° ë‚ ì§œ, í™ˆ/ì–´ì›¨ì´ íŒ€ ì½”ë“œ ë“±ì˜ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ëœ ê²½ê¸° ì •ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, request_delay: float = 1.5):
        self.base_url = "https://www.koreabaseball.com/Schedule/Schedule.aspx"
        self.request_delay = request_delay

    async def crawl_schedule(self, year: int, month: int, series_id: str = None) -> List[Dict]:
        """
        ì§€ì •ëœ ì—°ë„ì™€ ì›”ì˜ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.

        Args:
            year: ì‹œì¦Œ ì—°ë„ (ì˜ˆ: 2024)
            month: ì›” (1-12)
            series_id: ì‹œë¦¬ì¦ˆ ID (ì˜µì…˜)

        Returns:
            ê²½ê¸° ì •ë³´ ë”•ì…”ë„ˆë¦¬ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸.
        """
        print(f"ğŸ” Crawling schedule for {year}-{month:02d} (Series: {series_id})...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                games = await self._crawl_month(page, year, month, series_id=series_id)
                print(f"âœ… Found {len(games)} games")
                return games
            except Exception as e:
                print(f"âŒ Error crawling schedule: {e}")
                return []
            finally:
                await browser.close()

    async def crawl_season(self, year: int, months: Optional[List[int]] = None) -> List[Dict]:
        """
        ì£¼ì–´ì§„ ì‹œì¦Œì˜ ì—¬ëŸ¬ ë‹¬ì— ê±¸ì³ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        Args:
            year: ì‹œì¦Œ ì—°ë„
            months: í¬ë¡¤ë§í•  ì›” ëª©ë¡ (ê¸°ë³¸ê°’: 3ì›”-10ì›”)
        """
        months = months or list(range(3, 11))
        all_games: List[Dict] = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                for month in months:
                    month_games = await self._crawl_month(page, year, month)
                    all_games.extend(month_games)
                return all_games
            finally:
                await browser.close()


    async def _crawl_month(self, page: Page, year: int, month: int, series_id: str = None) -> List[Dict]:
        """íŠ¹ì • ì›”ì˜ ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê²Œì„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ í˜ì´ì§€ë¡œ ì´ë™ (íŒŒë¼ë¯¸í„° ì—†ì´)
        if page.url != self.base_url:
            await page.goto(self.base_url, wait_until="networkidle", timeout=30000)
        
        print(f"[NAV] Selecting Year: {year}, Month: {month}, Series: {series_id}")

        # 1. ì—°ë„ ì„ íƒ
        await page.select_option('#ddlYear', str(year))
        await asyncio.sleep(0.5)

        # 2. ì›” ì„ íƒ 
        # (ì›” ì„ íƒ -> í¬ìŠ¤íŠ¸ë°±)
        await page.select_option('#ddlMonth', f"{month:02d}")
        try:
            await page.wait_for_timeout(500)
            await page.wait_for_load_state("networkidle", timeout=5000)
        except:
            pass
            
        # 3. ë¦¬ê·¸(Series) ì„ íƒ (ì˜µì…˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        # series_idê°€ ì œê³µë˜ë©´ ì„ íƒ. (ì˜ˆ: "0,9,6" for Regular, "1" for Exhibition)
        if series_id:
            try:
                # í•´ë‹¹ ê°’ì´ ì˜µì…˜ì— ìˆëŠ”ì§€ í™•ì¸
                option_exists = await page.eval_on_selector(f'#ddlSeries option[value="{series_id}"]', 'e => !!e')
                if option_exists:
                    await page.select_option('#ddlSeries', series_id)
                    # ì‹œë¦¬ì¦ˆ ì„ íƒ -> í¬ìŠ¤íŠ¸ë°±
                    try:
                        await page.wait_for_timeout(500)
                        await page.wait_for_load_state("networkidle", timeout=5000)
                    except:
                        pass
                else:
                    print(f"[WARN] Series option '{series_id}' not found for {year}-{month:02d}. Skipping series selection.")
            except Exception as e:
                print(f"[WARN] Error selecting series {series_id}: {e}")

        await asyncio.sleep(self.request_delay)
        
        return await self._extract_games(page, year, month)

    async def _extract_games(self, page: Page, year: int, month: int) -> List[Dict]:
        """í˜ì´ì§€ì—ì„œ ê²½ê¸° ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        `gameId`ê°€ í¬í•¨ëœ ëª¨ë“  ë§í¬ë¥¼ ì°¾ì•„, ê° ë§í¬ì—ì„œ ê²½ê¸° ID, ë‚ ì§œ, íŒ€ ì •ë³´ ë“±ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        games = []

        # `gameId` íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ëœ ëª¨ë“  ê²½ê¸° ë§í¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        game_links = await page.query_selector_all('a[href*="gameId="]')

        for link in game_links:
            try:
                href = await link.get_attribute('href')
                if not href or 'gameId=' not in href:
                    continue

                # URLì—ì„œ game_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                game_id = self._extract_game_id(href)
                if not game_id:
                    continue

                # game_id í˜•ì‹(YYYYMMDD...)ì„ ë°”íƒ•ìœ¼ë¡œ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                game_date = game_id[:8]

                # game_idì—ì„œ í™ˆ/ì–´ì›¨ì´ íŒ€ ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                away_segment = game_id[8:10] if len(game_id) >= 10 else None
                home_segment = game_id[10:12] if len(game_id) >= 12 else None

                games.append({
                    'game_id': game_id,
                    'game_date': game_date,
                    'season_year': year,
                    'season_type': 'regular', # ì‹œì¦Œ ìœ í˜• (ì •ê·œ, í¬ìŠ¤íŠ¸ì‹œì¦Œ ë“±)
                    'away_team_code': team_code_from_game_id_segment(away_segment, year),
                    'home_team_code': team_code_from_game_id_segment(home_segment, year),
                    'doubleheader_no': int(game_id[-1]) if game_id[-1].isdigit() else 0, # ë”ë¸”í—¤ë” ì—¬ë¶€
                    'game_status': 'scheduled', # ê²½ê¸° ìƒíƒœ (ì˜ˆì •, ì¢…ë£Œ ë“±)
                    'crawl_status': 'pending', # í¬ë¡¤ë§ ìƒíƒœ
                    'url': f"https://www.koreabaseball.com{href}" if href.startswith('/') else href
                })

            except Exception as e:
                print(f"[WARN] Error extracting game: {e}")
                continue

        # game_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µëœ ê²½ê¸° ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        unique_games = {g['game_id']: g for g in games}
        return list(unique_games.values())

    def _extract_game_id(self, href: str) -> str:
        """URL(href)ì—ì„œ game_idë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if 'gameId=' in href:
                game_id = href.split('gameId=')[1].split('&')[0]
                return game_id
        except:
            pass
        return ""


async def main():
    """Test the schedule crawler"""
    crawler = ScheduleCrawler()

    # Crawl current month schedule
    now = datetime.now()
    games = await crawler.crawl_schedule(now.year, now.month)

    print(f"\nğŸ“Š Schedule Summary:")
    print(f"Total games found: {len(games)}")

    if games:
        print(f"\nğŸ“ First 5 games:")
        for game in games[:5]:
            print(f"  - {game['game_id']} | {game['game_date']}")


if __name__ == "__main__":
    asyncio.run(main())
