"""
KBO Schedule Crawler POC
Collects game IDs from the KBO schedule page
"""
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import Page

from src.utils.team_codes import team_code_from_game_id_segment, resolve_team_code
from src.utils.playwright_pool import AsyncPlaywrightPool
from src.utils.compliance import compliance


class ScheduleCrawler:
    """KBO ê³µì‹ ì‚¬ì´íŠ¸ì˜ ì›”ë³„ ê²½ê¸° ì¼ì • í˜ì´ì§€ì—ì„œ ê²½ê¸° ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - íŠ¹ì • ì—°ë„ì™€ ì›”ì— í•´ë‹¹í•˜ëŠ” ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
    - í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ê²½ê¸° ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ìœ  ID(gameId)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - gameIdë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ê¸° ë‚ ì§œ, í™ˆ/ì–´ì›¨ì´ íŒ€ ì½”ë“œ ë“±ì˜ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ëœ ê²½ê¸° ì •ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, request_delay: float = 1.5, pool: Optional[AsyncPlaywrightPool] = None):
        self.base_url = "https://www.koreabaseball.com/Schedule/Schedule.aspx"
        self.request_delay = request_delay
        self.pool = pool

    async def crawl_schedule(self, year: int, month: int, series_id: str = None) -> List[Dict]:
        """
        ì§€ì •ëœ ì—°ë„ì™€ ì›”ì˜ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.

        Args:
            year: ì‹œì¦Œ ì—°ë„ (ì˜ˆ: 2024)
            month: ì›” (1-12)
            series_id: ì‹œë¦¬ì¦ˆ ID (ì˜µì…˜)

        Returns:
            ê²½ê¸° ì •ë³´ ë”•ì…”ë„ˆë¦¬ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸.
        """
        print(f"ğŸ” Crawling schedule for {year}-{month:02d} (Series: {series_id})...")

        pool = self.pool or AsyncPlaywrightPool(max_pages=1)
        owns_pool = self.pool is None
        await pool.start()
        try:
            page = await pool.acquire()
            try:
                games = await self._crawl_month(page, year, month, series_id=series_id)
                print(f"âœ… Found {len(games)} games")
                return games
            except Exception as e:
                print(f"âŒ Error crawling schedule: {e}")
                return []
            finally:
                await pool.release(page)
        finally:
            if owns_pool:
                await pool.close()

    async def crawl_season(self, year: int, months: Optional[List[int]] = None, series_id: str = None) -> List[Dict]:
        """
        ì£¼ì–´ì§„ ì‹œì¦Œì˜ ì—¬ëŸ¬ ë‹¬ì— ê±¸ì³ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        Args:
            year: ì‹œì¦Œ ì—°ë„
            months: í¬ë¡¤ë§í•  ì›” ëª©ë¡ (ê¸°ë³¸ê°’: 3ì›”-10ì›”)
            series_id: ì‹œë¦¬ì¦ˆ ID (ì˜µì…˜)
        """
        months = months or list(range(3, 11))
        all_games: List[Dict] = []

        pool = self.pool or AsyncPlaywrightPool(max_pages=1)
        owns_pool = self.pool is None
        await pool.start()
        try:
            page = await pool.acquire()
            try:
                for month in months:
                    month_games = await self._crawl_month(page, year, month, series_id=series_id)
                    all_games.extend(month_games)
                return all_games
            finally:
                await pool.release(page)
        finally:
            if owns_pool:
                await pool.close()


    async def _crawl_month(self, page: Page, year: int, month: int, series_id: str = None) -> List[Dict]:
        """íŠ¹ì • ì›”ì˜ ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê²Œì„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ í˜ì´ì§€ë¡œ ì´ë™ (íŒŒë¼ë¯¸í„° ì—†ì´)
        if page.url != self.base_url:
            if not await compliance.is_allowed(self.base_url):
                print(f"[COMPLIANCE] Navigation to {self.base_url} aborted.")
                return []
            await page.goto(self.base_url, wait_until="networkidle", timeout=30000)
        
        print(f"[NAV] Selecting Year: {year}, Month: {month}, Series: {series_id}")

        # 1. ì—°ë„ ì„ íƒ
        # Check if year is already selected
        current_year = await page.eval_on_selector('#ddlYear', 'el => el.value')
        if current_year != str(year):
            # Check if option exists
            has_year = await page.eval_on_selector(f'#ddlYear option[value="{year}"]', 'e => !!e')
            if not has_year:
                print(f"[WARN] Year {year} not found in dropdown options.")
                return []
                
            await page.select_option('#ddlYear', str(year))
            # Year change triggers postback
            try:
                await page.wait_for_load_state("networkidle", timeout=5000)
                await page.wait_for_timeout(1000) # Safety wait for JS re-init
            except:
                pass

        # 2. ì›” ì„ íƒ 
        current_month = await page.eval_on_selector('#ddlMonth', 'el => el.value')
        target_month_str = f"{month:02d}"
        
        if current_month != target_month_str:
            await page.select_option('#ddlMonth', target_month_str)
            try:
                await page.wait_for_load_state("networkidle", timeout=5000)
                await page.wait_for_timeout(500)
            except:
                pass
            
        # 3. ë¦¬ê·¸(Series) ì„ íƒ (ì˜µì…˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if series_id:
            try:
                # í•´ë‹¹ ê°’ì´ ì˜µì…˜ì— ìˆëŠ”ì§€ í™•ì¸
                option_exists = await page.eval_on_selector(f'#ddlSeries option[value="{series_id}"]', 'e => !!e')
                if option_exists:
                    await page.select_option('#ddlSeries', series_id)
                    # ì‹œë¦¬ì¦ˆ ì„ íƒ -> í¬ìŠ¤íŠ¸ë°±
                    try:
                        await page.wait_for_timeout(500)
                        await page.wait_for_load_state("networkidle", timeout=5000)
                    except:
                        pass
                else:
                    print(f"[WARN] Series option '{series_id}' not found for {year}-{month:02d}. Skipping series selection.")
            except Exception as e:
                print(f"[WARN] Error selecting series {series_id}: {e}")

        await asyncio.sleep(self.request_delay)
        
        return await self._extract_games(page, year, month)

    async def _extract_games(self, page: Page, year: int, month: int) -> List[Dict]:
        """í˜ì´ì§€ì—ì„œ ê²½ê¸° ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (JS Fast Path)

        `gameId`ê°€ í¬í•¨ëœ ëª¨ë“  ë§í¬ë¥¼ ì°¾ì•„, ê° ë§í¬ì—ì„œ ê²½ê¸° ID, ë‚ ì§œ, íŒ€ ì •ë³´ ë“±ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        
        # JSë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê²Œì„ ì •ë³´ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
        extraction_script = """
        (year) => {
            const results = [];
            const rows = document.querySelectorAll('.tbl tbody tr');
            let currentDateString = ""; // To handle rowspan or implicit date

            rows.forEach(tr => {
                // If it's a "No Game" row, skip
                if (tr.innerText.includes("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")) return;

                const cells = Array.from(tr.querySelectorAll('td'));
                if (cells.length < 3) return;

                // 1. Identify Date
                // Sometimes the date is in the first cell: "03.28(Sat)"
                // Or proper class might be used. 
                // Let's check the first cell's text.
                let firstCellText = cells[0].innerText.trim();
                let timeCellIndex = 1;
                let matchCellIndex = 2;
                let stadiumCellIndex = 7; // Default assumption based on sample

                // heuristic: Date like "03.28" or "03.28(í† )"
                // If first cell matches date pattern, update currentDateString
                // Regex: DD.MM or MM.DD? KBO is usually MM.DD(Day)
                const dateMatch = firstCellText.match(/(\d{2})\.(\d{2})/);
                if (dateMatch) {
                    currentDateString = dateMatch[0]; // "03.28"
                    // If date cell exists, time is next
                } else {
                    // If no date in first cell, it might be rowspan'd from previous.
                    // But effectively in DOM traversal, if a cell is rowspan'd, it doesn't appear in subsequent rows?
                    // Actually, in `querySelectorAll` of TRs, if a TD has rowspan=5, it only appears in the FIRST TR.
                    // subsequent TRs will have fewer cells.
                    // So we must rely on `currentDateString` persistence.
                    // AND adjust indices. If date cell is missing, Time is likely at index 0.
                    
                    // Check if first cell is Time
                    if (/^\d{1,2}:\d{2}$/.test(firstCellText)) {
                        timeCellIndex = 0;
                        matchCellIndex = 1;
                        stadiumCellIndex = 6; // Shifted by 1?
                    }
                }
                
                if (!currentDateString) return; // Can't parse without date

                // 2. Extract Time
                const timeText = cells[timeCellIndex] ? cells[timeCellIndex].innerText.trim() : "";
                if (!/^\d{1,2}:\d{2}$/.test(timeText)) return; // Not a game row?

                // 3. Extract Matchup (Away vs Home)
                // Text: "KTvsLG" or "KT vs LG"
                const matchText = cells[matchCellIndex] ? cells[matchCellIndex].innerText.trim() : "";
                if (!matchText.includes("vs")) return;

                const teams = matchText.split("vs");
                if (teams.length !== 2) return;
                
                const awayName = teams[0].trim();
                const homeName = teams[1].trim();

                // 4. Extract Stadium (heuristic index)
                // Based on sample: ['03.28(í† )\t14:00\tKTvsLG\t\t\t\t\tì ì‹¤\t-']
                // Split by tab shows many empty cells. 
                // Let's just find the cell that is NOT Time, NOT Match, and looks like Stadium.
                // Stadiums: ì ì‹¤, ë¬¸í•™, ëŒ€êµ¬, ì°½ì›, ëŒ€ì „, ê³ ì²™, ê´‘ì£¼, ì‚¬ì§, ìˆ˜ì›
                let stadium = "";
                for (let i = matchCellIndex + 1; i < cells.length; i++) {
                    const txt = cells[i].innerText.trim();
                    if (txt.length >= 2 && txt.length <= 5 && !txt.includes("-") && !txt.includes("ì·¨ì†Œ")) {
                         // Very rough heuristic
                         stadium = txt;
                         break;
                    }
                }
                
                // 5. Construct Game ID
                // Need standard team codes. The site uses Names (KT, LG, etc.).
                // We need to map Name -> Code. 
                // Ideally this mapping happens in Python, but we need Code to form GameID if we want it in JS.
                // OR we pass raw names to Python and Python constructs ID.
                // Let's pass raw names and let Python handle ID construction if ID is missing.
                
                // Let's check if there is a link
                const link = tr.querySelector('a[href*="gameId="]');
                if (link) {
                    // We already handled this in previous logic? 
                    // No, we are replacing the logic or augmenting it?
                    // The instruction says "Update the JS... to fallback".
                    // So I should keep the link logic or merge it.
                    // If link exists, we prefer it.
                    return; 
                }

                // If no link, we construct data
                // Date extraction: "03.28" -> "20260328"
                const [mm, dd] = currentDateString.split(".");
                const fullDate = `${year}${mm}${dd}`;

                results.push({
                    game_id: null,
                    game_date: fullDate,
                    season_year: year,
                    season_type: 'regular',
                    away_name: awayName,
                    home_name: homeName,
                    doubleheader_no: 0,
                    game_status: 'scheduled',
                    crawl_status: 'text_parsed',
                    url_suffix: '', 
                    game_time: timeText,
                    stadium: stadium
                });
            });

            // Original Link-based Logic (keep it for existing games)
            const links = document.querySelectorAll('a[href*="gameId="]');
            links.forEach(link => {
                const href = link.getAttribute('href');
                const match = href.match(/gameId=([^&]+)/);
                if (!match) return;
                const gameId = match[1];
                
                // ... same parsing ...
                const gameDate = gameId.substring(0, 8);
                const awaySegment = gameId.length >= 10 ? gameId.substring(8, 10) : "";
                const homeSegment = gameId.length >= 12 ? gameId.substring(10, 12) : "";
                const doubleHeader = (!isNaN(parseInt(gameId.slice(-1)))) ? parseInt(gameId.slice(-1)) : 0;
                
                // Time/Stadium extraction from DOM (link parent)
                let gameTime = null;
                let stadium = null;
                try {
                    const cell = link.closest('td');
                    if (cell) {
                         const row = cell.parentElement; 
                         // Try to find time in the row
                         const cells = Array.from(row.querySelectorAll('td'));
                         cells.forEach(c => {
                            const t = c.innerText.trim();
                            if (/^\\d{1,2}:\\d{2}$/.test(t)) gameTime = t;
                         });
                         // Stadium...
                         // Reuse the loop or index logic?
                         // It's consistent with text-only logic.
                    }
                } catch(e) {}

                results.push({
                    game_id: gameId,
                    game_date: gameDate,
                    season_year: year,
                    season_type: 'regular',
                    away_segment: awaySegment, 
                    home_segment: homeSegment,
                    doubleheader_no: doubleHeader,
                    game_status: 'scheduled',
                    crawl_status: 'link_parsed',
                    url_suffix: href,
                    game_time: gameTime,
                    stadium: stadium
                });
            });

            return results;
        }
        """

        try:
            raw_games = await page.evaluate(extraction_script, year)
            games = []

            for g in raw_games:
                away_code = team_code_from_game_id_segment(g.get('away_segment'), year)
                home_code = team_code_from_game_id_segment(g.get('home_segment'), year)
                
                # Fallback Construction if game_id is missing (future games or link not found)
                if not g.get('game_id'):
                    away_name = g.get('away_name')
                    home_name = g.get('home_name')
                    away_code = resolve_team_code(away_name) or away_name
                    home_code = resolve_team_code(home_name) or home_name
                    
                    # KBO Website uses LEGACY codes in Game IDs.
                    # We must map our canonical codes (KH, DB, SSG, KIA) to KBO legacy (WO, OB, SK, HT).
                    KBO_LEGACY_CODES = {
                        "KH": "WO",  # Kiwoom -> Woori
                        "DB": "OB",  # Doosan -> OB
                        "SSG": "SK", # SSG -> SK (Wyverns)
                        "KIA": "HT", # KIA -> Haitai
                        # Others usually match: LG, LT, NC, HH, KT, SS
                    }
                    
                    kbo_away_code = KBO_LEGACY_CODES.get(away_code, away_code)
                    kbo_home_code = KBO_LEGACY_CODES.get(home_code, home_code)

                    if g.get('game_date') and kbo_away_code and kbo_home_code:
                        # Construct ID: YYYYMMDD + AWAY + HOME + DH
                        # Doubleheader logic: If DH exists, use it. Default 0.
                        dh = g.get('doubleheader_no', 0)
                        
                        constructed_id = f"{g['game_date']}{kbo_away_code}{kbo_home_code}{dh}"
                        g['game_id'] = constructed_id                
                games.append({
                    'game_id': g['game_id'],
                    'game_date': g['game_date'],
                    'season_year': g['season_year'],
                    'season_type': g['season_type'],
                    'away_team_code': away_code,
                    'home_team_code': home_code,
                    'doubleheader_no': g['doubleheader_no'],
                    'game_status': g['game_status'],
                    'crawl_status': g['crawl_status'],
                    'game_time': g.get('game_time'),
                    'stadium': g.get('stadium'),
                    'url': f"https://www.koreabaseball.com{g['url_suffix']}" if g.get('url_suffix') and g['url_suffix'].startswith('/') else g.get('url_suffix')
                })
            
            
        except Exception as e:
            print(f"[WARN] Error extracting game (JS): {e}")
            return []
            
        if not games:
             # Debugging: Check if table exists or content
             content = await page.content()
             print(f"[DEBUG] No games found. Page content len: {len(content)}")
             if "gameId=" in content:
                 print("[DEBUG] 'gameId=' string FOUND in HTML but extraction failed.")
             else:
                 print("[DEBUG] 'gameId=' string NOT found in HTML.")
                 # Dump first few rows of the table to see structure
                 debug_script = """
                 () => {
                     const rows = document.querySelectorAll('.tbl tbody tr');
                     const data = [];
                     for(let i=0; i<Math.min(rows.length, 5); i++) {
                         data.push(rows[i].innerText);
                     }
                     return data;
                 }
                 """
                 try:
                     rows_text = await page.evaluate(debug_script)
                     print(f"[DEBUG] Table Rows Sample: {rows_text}")
                 except:
                     pass
                 
        return games

    def _extract_game_id(self, href: str) -> str:
        """URL(href)ì—ì„œ game_idë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if 'gameId=' in href:
                game_id = href.split('gameId=')[1].split('&')[0]
                return game_id
        except:
            pass
        return ""


async def main():
    """Test the schedule crawler"""
    crawler = ScheduleCrawler()

    # Crawl current month schedule
    now = datetime.now()
    games = await crawler.crawl_schedule(now.year, now.month)

    print(f"\nğŸ“Š Schedule Summary:")
    print(f"Total games found: {len(games)}")

    if games:
        print(f"\nğŸ“ First 5 games:")
        for game in games[:5]:
            print(f"  - {game['game_id']} | {game['game_date']}")


if __name__ == "__main__":
    asyncio.run(main())
