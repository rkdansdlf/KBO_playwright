"""
KBO Schedule Crawler POC
Collects game IDs from the KBO schedule page
"""
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import Page

from src.utils.team_codes import team_code_from_game_id_segment
from src.utils.playwright_pool import AsyncPlaywrightPool


class ScheduleCrawler:
    """KBO ê³µì‹ ì‚¬ì´íŠ¸ì˜ ì›”ë³„ ê²½ê¸° ì¼ì • í˜ì´ì§€ì—ì„œ ê²½ê¸° ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - íŠ¹ì • ì—°ë„ì™€ ì›”ì— í•´ë‹¹í•˜ëŠ” ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
    - í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ê²½ê¸° ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ìœ  ID(gameId)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - gameIdë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ê¸° ë‚ ì§œ, í™ˆ/ì–´ì›¨ì´ íŒ€ ì½”ë“œ ë“±ì˜ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ëœ ê²½ê¸° ì •ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, request_delay: float = 1.5, pool: Optional[AsyncPlaywrightPool] = None):
        self.base_url = "https://www.koreabaseball.com/Schedule/Schedule.aspx"
        self.request_delay = request_delay
        self.pool = pool

    async def crawl_schedule(self, year: int, month: int, series_id: str = None) -> List[Dict]:
        """
        ì§€ì •ëœ ì—°ë„ì™€ ì›”ì˜ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.

        Args:
            year: ì‹œì¦Œ ì—°ë„ (ì˜ˆ: 2024)
            month: ì›” (1-12)
            series_id: ì‹œë¦¬ì¦ˆ ID (ì˜µì…˜)

        Returns:
            ê²½ê¸° ì •ë³´ ë”•ì…”ë„ˆë¦¬ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸.
        """
        print(f"ğŸ” Crawling schedule for {year}-{month:02d} (Series: {series_id})...")

        pool = self.pool or AsyncPlaywrightPool(max_pages=1)
        owns_pool = self.pool is None
        await pool.start()
        try:
            page = await pool.acquire()
            try:
                games = await self._crawl_month(page, year, month, series_id=series_id)
                print(f"âœ… Found {len(games)} games")
                return games
            except Exception as e:
                print(f"âŒ Error crawling schedule: {e}")
                return []
            finally:
                await pool.release(page)
        finally:
            if owns_pool:
                await pool.close()

    async def crawl_season(self, year: int, months: Optional[List[int]] = None) -> List[Dict]:
        """
        ì£¼ì–´ì§„ ì‹œì¦Œì˜ ì—¬ëŸ¬ ë‹¬ì— ê±¸ì³ ê²½ê¸° ì¼ì •ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        Args:
            year: ì‹œì¦Œ ì—°ë„
            months: í¬ë¡¤ë§í•  ì›” ëª©ë¡ (ê¸°ë³¸ê°’: 3ì›”-10ì›”)
        """
        months = months or list(range(3, 11))
        all_games: List[Dict] = []

        pool = self.pool or AsyncPlaywrightPool(max_pages=1)
        owns_pool = self.pool is None
        await pool.start()
        try:
            page = await pool.acquire()
            try:
                for month in months:
                    month_games = await self._crawl_month(page, year, month)
                    all_games.extend(month_games)
                return all_games
            finally:
                await pool.release(page)
        finally:
            if owns_pool:
                await pool.close()


    async def _crawl_month(self, page: Page, year: int, month: int, series_id: str = None) -> List[Dict]:
        """íŠ¹ì • ì›”ì˜ ê²½ê¸° ì¼ì • í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê²Œì„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ í˜ì´ì§€ë¡œ ì´ë™ (íŒŒë¼ë¯¸í„° ì—†ì´)
        if page.url != self.base_url:
            await page.goto(self.base_url, wait_until="networkidle", timeout=30000)
        
        print(f"[NAV] Selecting Year: {year}, Month: {month}, Series: {series_id}")

        # 1. ì—°ë„ ì„ íƒ
        await page.select_option('#ddlYear', str(year))
        await asyncio.sleep(0.5)

        # 2. ì›” ì„ íƒ 
        # (ì›” ì„ íƒ -> í¬ìŠ¤íŠ¸ë°±)
        await page.select_option('#ddlMonth', f"{month:02d}")
        try:
            await page.wait_for_timeout(500)
            await page.wait_for_load_state("networkidle", timeout=5000)
        except:
            pass
            
        # 3. ë¦¬ê·¸(Series) ì„ íƒ (ì˜µì…˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        # series_idê°€ ì œê³µë˜ë©´ ì„ íƒ. (ì˜ˆ: "0,9,6" for Regular, "1" for Exhibition)
        if series_id:
            try:
                # í•´ë‹¹ ê°’ì´ ì˜µì…˜ì— ìˆëŠ”ì§€ í™•ì¸
                option_exists = await page.eval_on_selector(f'#ddlSeries option[value="{series_id}"]', 'e => !!e')
                if option_exists:
                    await page.select_option('#ddlSeries', series_id)
                    # ì‹œë¦¬ì¦ˆ ì„ íƒ -> í¬ìŠ¤íŠ¸ë°±
                    try:
                        await page.wait_for_timeout(500)
                        await page.wait_for_load_state("networkidle", timeout=5000)
                    except:
                        pass
                else:
                    print(f"[WARN] Series option '{series_id}' not found for {year}-{month:02d}. Skipping series selection.")
            except Exception as e:
                print(f"[WARN] Error selecting series {series_id}: {e}")

        await asyncio.sleep(self.request_delay)
        
        return await self._extract_games(page, year, month)

    async def _extract_games(self, page: Page, year: int, month: int) -> List[Dict]:
        """í˜ì´ì§€ì—ì„œ ê²½ê¸° ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (JS Fast Path)

        `gameId`ê°€ í¬í•¨ëœ ëª¨ë“  ë§í¬ë¥¼ ì°¾ì•„, ê° ë§í¬ì—ì„œ ê²½ê¸° ID, ë‚ ì§œ, íŒ€ ì •ë³´ ë“±ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        
        # JSë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê²Œì„ ì •ë³´ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
        extraction_script = """
        (year) => {
            const links = document.querySelectorAll('a[href*="gameId="]');
            const results = [];
            const seenIds = new Set();

            links.forEach(link => {
                const href = link.getAttribute('href');
                if (!href) return;
                
                // Extract gameId from href
                const match = href.match(/gameId=([^&]+)/);
                if (!match) return;
                
                const gameId = match[1];
                if (seenIds.has(gameId)) return;
                seenIds.add(gameId);
                
                // Parse date and teams from gameId
                // Format: YYYYMMDD...
                const gameDate = gameId.substring(0, 8);
                const awaySegment = gameId.length >= 10 ? gameId.substring(8, 10) : "";
                const homeSegment = gameId.length >= 12 ? gameId.substring(10, 12) : "";
                const doubleHeader = (!isNaN(parseInt(gameId.slice(-1)))) ? parseInt(gameId.slice(-1)) : 0;

                results.push({
                    game_id: gameId,
                    game_date: gameDate,
                    season_year: year,
                    season_type: 'regular',
                    away_segment: awaySegment,
                    home_segment: homeSegment,
                    doubleheader_no: doubleHeader,
                    game_status: 'scheduled',
                    crawl_status: 'pending',
                    url_suffix: href
                });
            });
            return results;
        }
        """

        try:
            raw_games = await page.evaluate(extraction_script, year)
            games = []

            for g in raw_games:
                # Python-side processing for complex team codes if needed 
                # (although team_code_from_game_id_segment is simple, keeping it consistent)
                games.append({
                    'game_id': g['game_id'],
                    'game_date': g['game_date'],
                    'season_year': g['season_year'],
                    'season_type': g['season_type'],
                    'away_team_code': team_code_from_game_id_segment(g['away_segment'], year),
                    'home_team_code': team_code_from_game_id_segment(g['home_segment'], year),
                    'doubleheader_no': g['doubleheader_no'],
                    'game_status': g['game_status'],
                    'crawl_status': g['crawl_status'],
                    'url': f"https://www.koreabaseball.com{g['url_suffix']}" if g['url_suffix'].startswith('/') else g['url_suffix']
                })
            
            return games

        except Exception as e:
            print(f"[WARN] Error extracting game (JS): {e}")
            return []

    def _extract_game_id(self, href: str) -> str:
        """URL(href)ì—ì„œ game_idë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if 'gameId=' in href:
                game_id = href.split('gameId=')[1].split('&')[0]
                return game_id
        except:
            pass
        return ""


async def main():
    """Test the schedule crawler"""
    crawler = ScheduleCrawler()

    # Crawl current month schedule
    now = datetime.now()
    games = await crawler.crawl_schedule(now.year, now.month)

    print(f"\nğŸ“Š Schedule Summary:")
    print(f"Total games found: {len(games)}")

    if games:
        print(f"\nğŸ“ First 5 games:")
        for game in games[:5]:
            print(f"  - {game['game_id']} | {game['game_date']}")


if __name__ == "__main__":
    asyncio.run(main())
