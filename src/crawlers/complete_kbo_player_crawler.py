"""
KBO ì„ ìˆ˜ íƒ€ì ê¸°ë¡ ì™„ì „ í¬ë¡¤ëŸ¬
Docs/schema/player_season_data.md ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„

ì •ê·œì‹œì¦Œ: Basic1 + Basic2 (í—¤ë” í´ë¦­) ë°ì´í„° ìˆ˜ì§‘
ê¸°íƒ€ì‹œë¦¬ì¦ˆ: Basic1 ê¸°ë³¸ ë°ì´í„°ë§Œ ìˆ˜ì§‘
"""

import sys
import os
import time
from typing import Dict, List, Optional, Union
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from playwright.sync_api import sync_playwright, Page
from src.repositories.save_futures_batting import save_futures_batting_stats
from src.db.engine import create_engine_for_url

def safe_parse_number(value_str: str, data_type: type, allow_zero: bool = True) -> Optional[Union[int, float]]:
    """ì•ˆì „í•œ ìˆ«ì íŒŒì‹± (0ê°’ ë³´ì¡´)"""
    if not value_str:
        return None
    value_str = value_str.strip()
    if not value_str or value_str in ['-', 'N/A', '']:
        return None
    try:
        parsed_value = data_type(value_str)
        return parsed_value
    except (ValueError, TypeError):
        return None

def parse_player_id_from_link(link_href: str) -> Optional[int]:
    """ë§í¬ì—ì„œ player_id ì¶”ì¶œ"""
    try:
        if 'playerId=' in link_href:
            player_id_str = link_href.split('playerId=')[1].split('&')[0]
            return int(player_id_str)
    except (ValueError, IndexError):
        pass
    return None

def crawl_regular_season_data(page: Page, year: int) -> Dict[int, Dict]:
    """
    ì •ê·œì‹œì¦Œ ë°ì´í„° í¬ë¡¤ë§ (Basic1 + Basic2)
    ì»¬ëŸ¼: AVG,G,PA,AB,R,H,2B,3B,HR,TB,RBI,SAC,SF + BB,IBB,HBP,SO,GDP,SLG,OBP,OPS,MH,RISP,PH-BA
    """
    print(f"ğŸ“Š {year}ë…„ ì •ê·œì‹œì¦Œ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
    
    # 1. Basic1 ë°ì´í„° ìˆ˜ì§‘
    basic1_data = crawl_basic1_data(page, year, {'value': '0', 'name': 'ì •ê·œì‹œì¦Œ'})
    print(f"   âœ… Basic1 ë°ì´í„°: {len(basic1_data)}ëª…")
    
    # 2. Basic2 ë°ì´í„° ìˆ˜ì§‘ (í—¤ë” í´ë¦­)
    basic2_data = crawl_basic2_with_headers(page, year, {'value': '0', 'name': 'ì •ê·œì‹œì¦Œ'})
    print(f"   âœ… Basic2 ë°ì´í„°: {len(basic2_data)}ëª…")
    
    # 3. ë°ì´í„° ë³‘í•©
    merged_data = {}
    for player_id in basic1_data.keys():
        merged_data[player_id] = {
            **basic1_data[player_id],
            **basic2_data.get(player_id, {})
        }
    
    # Basic2ì—ë§Œ ìˆëŠ” ì„ ìˆ˜ë“¤ë„ ì¶”ê°€
    for player_id in basic2_data.keys():
        if player_id not in merged_data:
            merged_data[player_id] = basic2_data[player_id]
    
    print(f"   âœ… ë³‘í•© ì™„ë£Œ: {len(merged_data)}ëª…")
    return merged_data

def crawl_basic1_data(page: Page, year: int, series_info: Dict) -> Dict[int, Dict]:
    """
    Basic1 í˜ì´ì§€ ë°ì´í„° í¬ë¡¤ë§
    ì»¬ëŸ¼: AVG,G,PA,AB,R,H,2B,3B,HR,TB,RBI,SAC,SF (ì •ê·œì‹œì¦Œ)
    ì»¬ëŸ¼: AVG,G,PA,AB,H,2B,3B,HR,RBI,SB,CS,BB,HBP,SO,GDP,E (ê¸°íƒ€ì‹œë¦¬ì¦ˆ)
    """
    print(f"   ğŸ” Basic1 ë°ì´í„° ìˆ˜ì§‘: {series_info['name']}")
    
    try:
        # Basic1 í˜ì´ì§€ë¡œ ì´ë™
        url = "https://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx"
        page.goto(url, wait_until='load', timeout=30000)
        page.wait_for_load_state('networkidle', timeout=30000)
        time.sleep(2)
        
        # ì—°ë„ ì„ íƒ
        season_selector = 'select[name="ctl00$ctl00$ctl00$cphContents$cphContents$cphContents$ddlSeason$ddlSeason"]'
        page.select_option(season_selector, str(year))
        time.sleep(1)
        
        # ì‹œë¦¬ì¦ˆ ì„ íƒ
        series_selector = 'select[name="ctl00$ctl00$ctl00$cphContents$cphContents$cphContents$ddlSeries$ddlSeries"]'
        page.select_option(series_selector, value=series_info['value'])
        time.sleep(2)
        
        # í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘
        all_player_data = {}
        page_num = 1
        
        while True:
            print(f"      ğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
            
            # í…Œì´ë¸” ì°¾ê¸°
            table = page.query_selector("table")
            if not table:
                print(f"      âš ï¸ í˜ì´ì§€ {page_num}ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # í—¤ë” í™•ì¸
            thead = table.query_selector("thead")
            if thead:
                header_cells = thead.query_selector_all("th")
                headers = [cell.inner_text().strip() for cell in header_cells]
                print(f"         ğŸ“‹ í—¤ë”: {headers}")
            
            # ë°ì´í„° í–‰ ì²˜ë¦¬
            tbody = table.query_selector("tbody")
            if tbody:
                rows = tbody.query_selector_all("tr")
            else:
                rows = table.query_selector_all("tr")[1:]  # ì²« ë²ˆì§¸ í–‰(í—¤ë”) ì œì™¸
            
            if not rows:
                print(f"      âš ï¸ í˜ì´ì§€ {page_num}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            for row in rows:
                cells = row.query_selector_all("td")
                if len(cells) < 5:  # ìµœì†Œ ë°ì´í„° í™•ì¸
                    continue
                
                # ì„ ìˆ˜ ë§í¬ì—ì„œ player_id ì¶”ì¶œ
                name_cell = cells[1] if len(cells) > 1 else None
                if not name_cell:
                    continue
                
                player_link = name_cell.query_selector("a")
                if not player_link:
                    continue
                
                player_id = parse_player_id_from_link(player_link.get_attribute("href"))
                if not player_id:
                    continue
                
                player_name = player_link.inner_text().strip()
                team_code = cells[2].inner_text().strip() if len(cells) > 2 else None
                
                # ê¸°ë³¸ ì •ë³´
                player_data = {
                    'player_id': player_id,
                    'player_name': player_name,
                    'team_code': team_code,
                    'year': year,
                    'league': 'KBO',
                    'source': 'profile',
                    'series_name': series_info['name'],
                    'series_value': series_info['value']
                }
                
                # ì‹œë¦¬ì¦ˆë³„ ì»¬ëŸ¼ ë§¤í•‘
                if series_info['value'] == '0':  # ì •ê·œì‹œì¦Œ
                    player_data.update(parse_regular_season_basic1_stats(cells))
                else:  # ê¸°íƒ€ ì‹œë¦¬ì¦ˆ
                    player_data.update(parse_other_series_stats(cells))
                
                all_player_data[player_id] = player_data
            
            print(f"         âœ… {len(rows)}ê°œ í–‰ ì²˜ë¦¬, ì´ {len(all_player_data)}ëª…")
            
            # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
            if not goto_next_page(page):
                break
            
            page_num += 1
            time.sleep(1)
        
        return all_player_data
        
    except Exception as e:
        print(f"   âŒ Basic1 ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

def parse_regular_season_basic1_stats(cells: List) -> Dict:
    """ì •ê·œì‹œì¦Œ Basic1 í†µê³„ íŒŒì‹±: AVG,G,PA,AB,R,H,2B,3B,HR,TB,RBI,SAC,SF"""
    stats = {}
    try:
        if len(cells) >= 16:  # ì˜ˆìƒ ì»¬ëŸ¼ ìˆ˜
            stats.update({
                'avg': safe_parse_number(cells[3].inner_text().strip(), float),
                'games': safe_parse_number(cells[4].inner_text().strip(), int),
                'plate_appearances': safe_parse_number(cells[5].inner_text().strip(), int),
                'at_bats': safe_parse_number(cells[6].inner_text().strip(), int),
                'runs': safe_parse_number(cells[7].inner_text().strip(), int),
                'hits': safe_parse_number(cells[8].inner_text().strip(), int),
                'doubles': safe_parse_number(cells[9].inner_text().strip(), int),
                'triples': safe_parse_number(cells[10].inner_text().strip(), int),
                'home_runs': safe_parse_number(cells[11].inner_text().strip(), int),
                'total_bases': safe_parse_number(cells[12].inner_text().strip(), int),
                'rbis': safe_parse_number(cells[13].inner_text().strip(), int),
                'sacrifice_bunts': safe_parse_number(cells[14].inner_text().strip(), int),
                'sacrifice_flies': safe_parse_number(cells[15].inner_text().strip(), int)
            })
    except Exception as e:
        print(f"      âš ï¸ ì •ê·œì‹œì¦Œ Basic1 í†µê³„ íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    return stats

def parse_other_series_stats(cells: List) -> Dict:
    """ê¸°íƒ€ì‹œë¦¬ì¦ˆ í†µê³„ íŒŒì‹±: AVG,G,PA,AB,H,2B,3B,HR,RBI,SB,CS,BB,HBP,SO,GDP,E"""
    stats = {}
    try:
        if len(cells) >= 18:  # ì˜ˆìƒ ì»¬ëŸ¼ ìˆ˜
            stats.update({
                'avg': safe_parse_number(cells[3].inner_text().strip(), float),
                'games': safe_parse_number(cells[4].inner_text().strip(), int),
                'plate_appearances': safe_parse_number(cells[5].inner_text().strip(), int),
                'at_bats': safe_parse_number(cells[6].inner_text().strip(), int),
                'hits': safe_parse_number(cells[7].inner_text().strip(), int),
                'doubles': safe_parse_number(cells[8].inner_text().strip(), int),
                'triples': safe_parse_number(cells[9].inner_text().strip(), int),
                'home_runs': safe_parse_number(cells[10].inner_text().strip(), int),
                'rbis': safe_parse_number(cells[11].inner_text().strip(), int),
                'stolen_bases': safe_parse_number(cells[12].inner_text().strip(), int),
                'caught_stealing': safe_parse_number(cells[13].inner_text().strip(), int),
                'walks': safe_parse_number(cells[14].inner_text().strip(), int),
                'hit_by_pitch': safe_parse_number(cells[15].inner_text().strip(), int),
                'strikeouts': safe_parse_number(cells[16].inner_text().strip(), int),
                'gdp': safe_parse_number(cells[17].inner_text().strip(), int),
                'errors': safe_parse_number(cells[18].inner_text().strip(), int) if len(cells) > 18 else None
            })
    except Exception as e:
        print(f"      âš ï¸ ê¸°íƒ€ì‹œë¦¬ì¦ˆ í†µê³„ íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    return stats

def crawl_basic2_with_headers(page: Page, year: int, series_info: Dict) -> Dict[int, Dict]:
    """
    Basic2 í—¤ë” í´ë¦­ìœ¼ë¡œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘
    ì»¬ëŸ¼: BB,IBB,HBP,SO,GDP,SLG,OBP,OPS,MH,RISP,PH-BA
    """
    print(f"   ğŸ” Basic2 í—¤ë” í´ë¦­ ë°ì´í„° ìˆ˜ì§‘: {series_info['name']}")
    
    headers_to_click = [
        ('BB', 'BB_CN', 'ë³¼ë„·'),
        ('IBB', 'IB_CN', 'ê³ ì˜ì‚¬êµ¬'),
        ('HBP', 'HP_CN', 'ì‚¬êµ¬'),
        ('SO', 'KK_CN', 'ì‚¼ì§„'),
        ('GDP', 'GD_CN', 'ë³‘ì‚´íƒ€'),
        ('SLG', 'SLG_RT', 'ì¥íƒ€ìœ¨'),
        ('OBP', 'OBP_RT', 'ì¶œë£¨ìœ¨'),
        ('OPS', 'OPS_RT', 'OPS'),
        ('MH', 'MH_HITTER_CN', 'ë©€í‹°íˆíŠ¸'),
        ('RISP', 'SP_HRA_RT', 'ë“ì ê¶Œíƒ€ìœ¨'),
        ('PH-BA', 'PH_HRA_RT', 'ëŒ€íƒ€íƒ€ìœ¨')
    ]
    
    all_player_data = {}
    
    try:
        # Basic1ì—ì„œ ì‹œì‘í•˜ì—¬ Basic2ë¡œ ì´ë™
        url = "https://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx"
        page.goto(url, wait_until='load', timeout=30000)
        page.wait_for_load_state('networkidle', timeout=30000)
        time.sleep(2)
        
        # ì—°ë„ ë° ì‹œë¦¬ì¦ˆ ì„ íƒ
        season_selector = 'select[name="ctl00$ctl00$ctl00$cphContents$cphContents$cphContents$ddlSeason$ddlSeason"]'
        page.select_option(season_selector, str(year))
        time.sleep(1)
        
        series_selector = 'select[name="ctl00$ctl00$ctl00$cphContents$cphContents$cphContents$ddlSeries$ddlSeries"]'
        page.select_option(series_selector, value=series_info['value'])
        time.sleep(2)
        
        # "ë‹¤ìŒ" ë§í¬ë¡œ Basic2 ì ‘ê·¼
        next_link = page.query_selector('a.next[href*="Basic2.aspx"]')
        if not next_link:
            print(f"      âš ï¸ Basic2 'ë‹¤ìŒ' ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        next_link.click()
        page.wait_for_load_state('networkidle', timeout=30000)
        time.sleep(2)
        
        # ê° í—¤ë”ë³„ ë°ì´í„° ìˆ˜ì§‘
        for i, (header_name, sort_code, description) in enumerate(headers_to_click):
            print(f"      ğŸ“Š {description}({header_name}) í—¤ë” í´ë¦­... ({i+1}/11)")
            
            try:
                # í—¤ë” í´ë¦­
                header_link = page.query_selector(f'a[href*="sort(\'{sort_code}\')"]')
                if not header_link:
                    print(f"         âš ï¸ {header_name} í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                header_link.click()
                page.wait_for_load_state('networkidle', timeout=30000)
                time.sleep(1)
                
                # í˜„ì¬ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
                page_data = collect_current_page_data(page, header_name)
                
                # ë°ì´í„° ë³‘í•©
                for player_id, data in page_data.items():
                    if player_id not in all_player_data:
                        all_player_data[player_id] = data
                    else:
                        all_player_data[player_id].update(data)
                
                print(f"         âœ… {len(page_data)}ëª… ë°ì´í„° ìˆ˜ì§‘")
                
            except Exception as e:
                print(f"         âŒ {header_name} í—¤ë” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"   âœ… Basic2 í—¤ë”ë³„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_player_data)}ëª…")
        return all_player_data
        
    except Exception as e:
        print(f"   âŒ Basic2 ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

def collect_current_page_data(page: Page, sort_field: str) -> Dict[int, Dict]:
    """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ì„ ìˆ˜ ë°ì´í„° ìˆ˜ì§‘"""
    page_data = {}
    
    try:
        # ëª¨ë“  í˜ì´ì§€ ìˆœíšŒ
        page_num = 1
        while True:
            # í…Œì´ë¸” ë°ì´í„° íŒŒì‹±
            table = page.query_selector("table")
            if not table:
                break
            
            tbody = table.query_selector("tbody")
            if tbody:
                rows = tbody.query_selector_all("tr")
            else:
                rows = table.query_selector_all("tr")[1:]
            
            if not rows:
                break
            
            for row in rows:
                cells = row.query_selector_all("td")
                if len(cells) < 5:
                    continue
                
                # player_id ì¶”ì¶œ
                name_cell = cells[1]
                player_link = name_cell.query_selector("a")
                if not player_link:
                    continue
                
                player_id = parse_player_id_from_link(player_link.get_attribute("href"))
                if not player_id:
                    continue
                
                # ê¸°ë³¸ ì •ë³´
                player_data = {
                    'player_id': player_id,
                    'player_name': player_link.inner_text().strip(),
                    'team_code': cells[2].inner_text().strip()
                }
                
                # sort_fieldì— ë”°ë¥¸ ë°ì´í„° ì¶”ì¶œ
                player_data.update(extract_basic2_stats(cells, sort_field))
                
                page_data[player_id] = player_data
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if not goto_next_page(page):
                break
            
            page_num += 1
            time.sleep(1)
    
    except Exception as e:
        print(f"         âš ï¸ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return page_data

def extract_basic2_stats(cells: List, sort_field: str) -> Dict:
    """Basic2 í†µê³„ ì¶”ì¶œ"""
    stats = {}
    try:
        # í—¤ë” ìœ„ì¹˜ ë§¤í•‘ (ì¶”ì •ê°’, ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        field_positions = {
            'BB': 4, 'IBB': 5, 'HBP': 6, 'SO': 7, 'GDP': 8,
            'SLG': 9, 'OBP': 10, 'OPS': 11, 'MH': 12, 'RISP': 13, 'PH-BA': 14
        }
        
        field_mapping = {
            'BB': 'walks',
            'IBB': 'intentional_walks', 
            'HBP': 'hit_by_pitch',
            'SO': 'strikeouts',
            'GDP': 'gdp',
            'SLG': 'slg',
            'OBP': 'obp', 
            'OPS': 'ops',
            'MH': 'multi_hits',
            'RISP': 'risp_avg',
            'PH-BA': 'pinch_hit_avg'
        }
        
        if sort_field in field_positions and sort_field in field_mapping:
            pos = field_positions[sort_field]
            field_name = field_mapping[sort_field]
            
            if len(cells) > pos:
                value_str = cells[pos].inner_text().strip()
                
                # ë°ì´í„° íƒ€ì… ê²°ì •
                if sort_field in ['SLG', 'OBP', 'OPS', 'RISP', 'PH-BA']:
                    data_type = float
                else:
                    data_type = int
                
                parsed_value = safe_parse_number(value_str, data_type)
                if parsed_value is not None:
                    stats[field_name] = parsed_value
                
    except Exception as e:
        print(f"         âš ï¸ Basic2 í†µê³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    return stats

def goto_next_page(page: Page) -> bool:
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
    try:
        # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
        pagination = page.query_selector(".paging")
        if not pagination:
            return False
        
        # "ë‹¤ìŒ" ë§í¬ ì°¾ê¸°
        next_links = pagination.query_selector_all("a")
        for link in next_links:
            if "ë‹¤ìŒ" in link.inner_text() or ">" in link.inner_text():
                href = link.get_attribute("href")
                if href and "javascript:" not in href:
                    link.click()
                    page.wait_for_load_state('networkidle', timeout=30000)
                    time.sleep(1)
                    return True
        
        return False
        
    except Exception as e:
        print(f"      âš ï¸ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def crawl_other_series_data(page: Page, year: int, series_list: List[Dict]) -> Dict[str, Dict[int, Dict]]:
    """ê¸°íƒ€ ì‹œë¦¬ì¦ˆ ë°ì´í„° í¬ë¡¤ë§ (ê¸°ë³¸ ë°ì´í„°ë§Œ)"""
    all_series_data = {}
    
    for series_info in series_list:
        print(f"ğŸ“Š {year}ë…„ {series_info['name']} ë°ì´í„° í¬ë¡¤ë§...")
        
        series_data = crawl_basic1_data(page, year, series_info)
        if series_data:
            all_series_data[series_info['name']] = series_data
            print(f"   âœ… {series_info['name']}: {len(series_data)}ëª…")
        else:
            print(f"   âš ï¸ {series_info['name']}: ë°ì´í„° ì—†ìŒ")
    
    return all_series_data

def save_to_database(player_data: Dict[int, Dict], series_name: str):
    """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        print(f"ğŸ’¾ {series_name} ë°ì´í„° ì €ì¥ ì¤‘...")
        
        saved_count = 0
        for player_id, data in player_data.items():
            try:
                # ì €ì¥ìš© ë°ì´í„° í˜•ì‹ ë³€í™˜
                save_data = {
                    'player_id': data['player_id'],
                    'year': data['year'],
                    'league': data['league'],
                    'source': data['source'],
                    'series_name': data['series_name'],
                    'team_code': data['team_code'],
                    'games': data.get('games'),
                    'plate_appearances': data.get('plate_appearances'),
                    'at_bats': data.get('at_bats'),
                    'runs': data.get('runs'),
                    'hits': data.get('hits'),
                    'doubles': data.get('doubles'),
                    'triples': data.get('triples'),
                    'home_runs': data.get('home_runs'),
                    'rbis': data.get('rbis'),
                    'walks': data.get('walks'),
                    'strikeouts': data.get('strikeouts'),
                    'avg': data.get('avg'),
                    'obp': data.get('obp'),
                    'slg': data.get('slg'),
                    'ops': data.get('ops'),
                    'stolen_bases': data.get('stolen_bases'),
                    'caught_stealing': data.get('caught_stealing'),
                    'hit_by_pitch': data.get('hit_by_pitch'),
                    'intentional_walks': data.get('intentional_walks'),
                    'sacrifice_bunts': data.get('sacrifice_bunts'),
                    'sacrifice_flies': data.get('sacrifice_flies'),
                    'gdp': data.get('gdp'),
                    'errors': data.get('errors'),
                    'total_bases': data.get('total_bases'),
                    'extra_stats': {
                        'multi_hits': data.get('multi_hits'),
                        'risp_avg': data.get('risp_avg'),
                        'pinch_hit_avg': data.get('pinch_hit_avg')
                    }
                }
                
                save_futures_batting_stats(save_data)
                saved_count += 1
                
            except Exception as e:
                print(f"   âš ï¸ {data['player_name']} ì €ì¥ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"   âœ… {saved_count}/{len(player_data)}ëª… ì €ì¥ ì™„ë£Œ")
        return saved_count
        
    except Exception as e:
        print(f"   âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ë§ ëŒ€ìƒ ì„¤ì •
    YEAR = 2025
    
    # ì‹œë¦¬ì¦ˆ ì •ì˜
    SERIES_LIST = [
        {'value': '1', 'name': 'KBO ì‹œë²”ê²½ê¸°'},
        {'value': '4', 'name': 'KBO ì™€ì¼ë“œì¹´ë“œ'},
        {'value': '3', 'name': 'KBO ì¤€í”Œë ˆì´ì˜¤í”„'},
        {'value': '5', 'name': 'KBO í”Œë ˆì´ì˜¤í”„'},
        {'value': '7', 'name': 'KBO í•œêµ­ì‹œë¦¬ì¦ˆ'}
    ]
    
    print(f"ğŸš€ KBO {YEAR}ë…„ ì„ ìˆ˜ íƒ€ì ê¸°ë¡ ì™„ì „ í¬ë¡¤ë§ ì‹œì‘")
    print(f"ğŸ“‹ ëŒ€ìƒ: ì •ê·œì‹œì¦Œ(Enhanced) + {len(SERIES_LIST)}ê°œ ì‹œë¦¬ì¦ˆ(Basic)")
    
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=False)
        page = browser.new_page()
        
        try:
            total_saved = 0
            
            # 1. ì •ê·œì‹œì¦Œ ë°ì´í„° ìˆ˜ì§‘ (Basic1 + Basic2)
            print(f"\n{'='*50}")
            print(f"ğŸ“Š 1ë‹¨ê³„: ì •ê·œì‹œì¦Œ ë°ì´í„° ìˆ˜ì§‘ (Enhanced)")
            print(f"{'='*50}")
            
            regular_season_data = crawl_regular_season_data(page, YEAR)
            if regular_season_data:
                saved = save_to_database(regular_season_data, "ì •ê·œì‹œì¦Œ")
                total_saved += saved
            
            # 2. ê¸°íƒ€ ì‹œë¦¬ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (Basic1ë§Œ)
            print(f"\n{'='*50}")
            print(f"ğŸ“Š 2ë‹¨ê³„: ê¸°íƒ€ ì‹œë¦¬ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (Basic)")
            print(f"{'='*50}")
            
            other_series_data = crawl_other_series_data(page, YEAR, SERIES_LIST)
            for series_name, series_data in other_series_data.items():
                if series_data:
                    saved = save_to_database(series_data, series_name)
                    total_saved += saved
            
            # 3. ìµœì¢… ê²°ê³¼
            print(f"\n{'='*50}")
            print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"{'='*50}")
            print(f"ğŸ“Š ì´ ì €ì¥ëœ ë ˆì½”ë“œ: {total_saved}ê°œ")
            print(f"ğŸ“… í¬ë¡¤ë§ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            browser.close()

if __name__ == "__main__":
    main()