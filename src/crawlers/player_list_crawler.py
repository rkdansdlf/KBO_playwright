"""
KBO Player List Crawler (Step 1)
Collects list of all players by team (hitters and pitchers)
"""
import asyncio
import time
from typing import List, Dict
from playwright.async_api import async_playwright, Page
from src.utils.safe_print import safe_print as print


class PlayerListCrawler:
    """KBO ê³µì‹ ê¸°ë¡ì‹¤ì—ì„œ íŠ¹ì • ì‹œì¦Œì˜ ëª¨ë“  íƒ€ìì™€ íˆ¬ìˆ˜ ëª©ë¡ì„ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ì§€ì •ëœ ì‹œì¦Œì˜ íƒ€ì ë° íˆ¬ìˆ˜ ìˆœìœ„ í˜ì´ì§€ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
    - ê° í˜ì´ì§€ì˜ ì„ ìˆ˜ í‘œì—ì„œ ì„ ìˆ˜ ì´ë¦„, íŒ€, ê³ ìœ  ID(playerId) ë“±ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - ëª¨ë“  ì„ ìˆ˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, request_delay: float = 1.5):
        self.base_url = "https://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx"
        self.pitcher_url = "https://www.koreabaseball.com/Record/Player/PitcherBasic/Basic1.aspx"
        self.request_delay = request_delay

        # KBO team codes
        self.teams = {
            'LG': 'LG íŠ¸ìœˆìŠ¤',
            'KT': 'KT ìœ„ì¦ˆ',
            'SK': 'SSG ëœë”ìŠ¤',
            'NC': 'NC ë‹¤ì´ë…¸ìŠ¤',
            'OB': 'ë‘ì‚° ë² ì–´ìŠ¤',
            'HH': 'í•œí™” ì´ê¸€ìŠ¤',
            'LT': 'ë¡¯ë° ìì´ì–¸ì¸ ',
            'SK': 'SK ì™€ì´ë²ˆìŠ¤',
            'HT': 'KIA íƒ€ì´ê±°ì¦ˆ',
            'SS': 'ì‚¼ì„± ë¼ì´ì˜¨ì¦ˆ'
        }

    async def crawl_all_players(self, season_year: int = 2024) -> Dict[str, List[Dict]]:
        """
        ì§€ì •ëœ ì‹œì¦Œì˜ ëª¨ë“  íƒ€ìì™€ íˆ¬ìˆ˜ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.

        Args:
            season_year: í¬ë¡¤ë§í•  ì‹œì¦Œ ì—°ë„ (ê¸°ë³¸ê°’: 2024)

        Returns:
            íƒ€ì(hitters)ì™€ íˆ¬ìˆ˜(pitchers) ëª©ë¡ì´ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬.
        """
        print(f"\nğŸ” Crawling all players for {season_year} season...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                # íƒ€ìì™€ íˆ¬ìˆ˜ ì •ë³´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
                all_hitters = await self._crawl_hitters(page, season_year)
                all_pitchers = await self._crawl_pitchers(page, season_year)

                return {
                    'hitters': all_hitters,
                    'pitchers': all_pitchers,
                    'season_year': season_year
                }

            except Exception as e:
                print(f"âŒ Error crawling players: {e}")
                return {'hitters': [], 'pitchers': [], 'season_year': season_year}
            finally:
                await browser.close()

    async def _crawl_hitters(self, page: Page, season_year: int) -> List[Dict]:
        """ëª¨ë“  íƒ€ì ëª©ë¡ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š Crawling hitters...")
        url = f"{self.base_url}?gyear={season_year}"

        await page.goto(url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(self.request_delay)

        hitters = await self._extract_player_table(page, 'hitter')
        print(f"âœ… Found {len(hitters)} hitters")

        return hitters

    async def _crawl_pitchers(self, page: Page, season_year: int) -> List[Dict]:
        """ëª¨ë“  íˆ¬ìˆ˜ ëª©ë¡ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š Crawling pitchers...")
        url = f"{self.pitcher_url}?gyear={season_year}"

        await page.goto(url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(self.request_delay)

        pitchers = await self._extract_player_table(page, 'pitcher')
        print(f"âœ… Found {len(pitchers)} pitchers")

        return pitchers

    async def _extract_player_table(self, page: Page, player_type: str) -> List[Dict]:
        """í˜ì´ì§€ ë‚´ì˜ ì„ ìˆ˜ ì •ë³´ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            page: Playwrightì˜ Page ê°ì²´.
            player_type: ì„ ìˆ˜ ìœ í˜• ('hitter' ë˜ëŠ” 'pitcher').

        Returns:
            ì¶”ì¶œëœ ì„ ìˆ˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        players = []

        try:
            # ì„ ìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ë©”ì¸ í…Œì´ë¸”ì„ ì°¾ìŠµë‹ˆë‹¤.
            # KBO ì‚¬ì´íŠ¸ëŠ” `tData01`, `tData02` ë“± ì—¬ëŸ¬ í´ë˜ìŠ¤ ì´ë¦„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ,
            # `div.record_result table`ê³¼ ê°™ì´ ë” ì‹ ë¢°ì„± ìˆëŠ” ì„ íƒìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            tables = await page.query_selector_all('div.record_result table, table[summary*="ì„ ìˆ˜"], table[class*="tData"]')

            if not tables:
                print(f"âš ï¸  No tables found for {player_type}")
                return players

            # ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ í…Œì´ë¸”ì´ ë©”ì¸ ì„ ìˆ˜ ëª©ë¡ì…ë‹ˆë‹¤.
            main_table = tables[0]
            rows = await main_table.query_selector_all('tbody tr')

            for row in rows:
                try:
                    cells = await row.query_selector_all('td')
                    if len(cells) < 3:
                        continue

                    # ì„ ìˆ˜ í”„ë¡œí•„ ë§í¬ì—ì„œ ê³ ìœ  ID(playerId)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    player_link = await row.query_selector('a[href*="playerId"]')
                    player_id = None
                    if player_link:
                        href = await player_link.get_attribute('href')
                        if href and 'playerId=' in href:
                            player_id = href.split('playerId=')[1].split('&')[0]

                    # ê° ì…€ì˜ í…ìŠ¤íŠ¸ ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    cell_values = []
                    for cell in cells:
                        text = await cell.inner_text()
                        cell_values.append(text.strip())

                    # ì„ ìˆ˜ ì´ë¦„ì´ ì—†ëŠ” í–‰ì€ ê±´ë„ˆëœë‹ˆë‹¤.
                    if not cell_values or len(cell_values) < 2:
                        continue

                    # ì¶”ì¶œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„ ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                    # ì»¬ëŸ¼ ìˆœì„œ: [ìˆœìœ„, ì„ ìˆ˜ëª…, íŒ€, ...]
                    player = {
                        'player_id': player_id,
                        'player_name': cell_values[1] if len(cell_values) > 1 else '',
                        'team': cell_values[2] if len(cell_values) > 2 else '',
                        'player_type': player_type,
                        'raw_data': cell_values
                    }

                    if player['player_name']:
                        players.append(player)

                except Exception as e:
                    print(f"âš ï¸  Error parsing player row: {e}")
                    continue

        except Exception as e:
            print(f"âŒ Error extracting {player_type} table: {e}")

        return players


async def main():
    """Test the player list crawler"""
    crawler = PlayerListCrawler()

    # Crawl all players for 2024 season
    result = await crawler.crawl_all_players(season_year=2024)

    print(f"\nğŸ“Š Player List Summary:")
    print(f"  Total Hitters: {len(result['hitters'])}")
    print(f"  Total Pitchers: {len(result['pitchers'])}")

    if result['hitters']:
        print(f"\n  Sample Hitters:")
        for player in result['hitters'][:5]:
            print(f"    - {player['player_name']} ({player['team']}) [ID: {player['player_id']}]")

    if result['pitchers']:
        print(f"\n  Sample Pitchers:")
        for player in result['pitchers'][:5]:
            print(f"    - {player['player_name']} ({player['team']}) [ID: {player['player_id']}]")


if __name__ == "__main__":
    asyncio.run(main())
